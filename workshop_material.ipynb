{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demystifying Neural Networks\n",
    "\n",
    "**A presentation by the Data Science and Analytics Club at UVA for HooHacks 2024** \n",
    "\n",
    "Workshop by: Maajid Husain :: [LinkedIn](https://www.linkedin.com/in/maajid-husain/) :: [Github](https://github.com/maajidhusain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 17:07:05.210563: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "We are running a superhero company and our new recruits need names. Luckily we have sourced a list of names from some of the most famous superheros from organizations like Marvel and DC Comics. Based on the foundational knowledge from the workshop today, you have been tasked with creating a `RNN` that is able to generate super hero names. Although generative AI can also solve this problem we want something less computationally taxing and an in house built application to solve this problem.\n",
    "\n",
    "An overview of how to solve the problem:\n",
    "1. Prepare Data\n",
    "2. Decide on model architecture\n",
    "3. Train and validate the model\n",
    "5. Generate names\n",
    "\n",
    "*If you want to go further, creating a web app that can automatically generate names on the click of a button is a great way to start adding neural networks to your project portfolio!!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Aquisition and Cleaning\n",
    "- Load dataset <https://gist.github.com/agarcas26/096346aa7a180d57f037cdb5a3f1418b>\n",
    "- Normalize text (all lowercase)\n",
    "- Character encoding: convert characters --> integers for model to process\n",
    "- Generate sequences of characters that wil be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Maajid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Maajid/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Maajid/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/Maajid/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation to add more rows to the data since we have a small dataset\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from random import randint\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonyms.add(synonym)\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(sentence, n):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in nltk.corpus.stopwords.words('english')]))\n",
    "    num_replaced = 0\n",
    "    for _ in range(n):\n",
    "        if len(random_word_list) == 0:\n",
    "            break\n",
    "        random_word = random_word_list[randint(0, len(random_word_list)-1)]\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = synonyms[0]\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        random_word_list.remove(random_word)\n",
    "    sentence = ' '.join(new_words)\n",
    "    return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters: 25228\n",
      "Total Vocab: 37\n",
      "Max Name Length: 65\n",
      "Total Patterns: 4961\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"superheroes.csv\")\n",
    "\n",
    "# Convert names to lowercase\n",
    "names = data['name'].str.lower().tolist()\n",
    "\n",
    "for name in names.copy():  # Iterate over a copy of the list to avoid modifying it while iterating\n",
    "    augmented_name1 = synonym_replacement(name, 2)  # Adjust the number of synonyms replaced as needed\n",
    "    augmented_name2 = synonym_replacement(augmented_name1, 2)\n",
    "    names.append(augmented_name1)  # Append the augmented name to the original list\n",
    "    names.append(augmented_name2)\n",
    "\n",
    "# Create a mapping of unique characters to integers\n",
    "chars = sorted(list(set(\"\".join(names))))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Calculate the vocabulary size and max length of names\n",
    "n_chars = sum([len(name) for name in names])\n",
    "n_vocab = len(chars)\n",
    "max_length = max([len(name) for name in names])\n",
    "\n",
    "print(f\"Total Characters: {n_chars}\")\n",
    "print(f\"Total Vocab: {n_vocab}\")\n",
    "print(f\"Max Name Length: {max_length}\")\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "# Sequence length for training can be smaller than the maximum to generate more patterns\n",
    "seq_length = 12  # This is an arbitrary choice, adjust based on your preference\n",
    "\n",
    "for name in names:\n",
    "    # Process each name to create sequences\n",
    "    for i in range(0, len(name) - seq_length, 1):\n",
    "        seq_in = name[i:i + seq_length]\n",
    "        seq_out = name[i + seq_length]\n",
    "        dataX.append([char_to_int[char] for char in seq_in])\n",
    "        dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(f\"Total Patterns: {n_patterns}\")\n",
    "\n",
    "# Update reshaping and normalization based on the new seq_length\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1)) / float(n_vocab)\n",
    "y = to_categorical(dataY)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build Model\n",
    "- Since an RNN is our neural network of choice, we will proceed with an LSTM\n",
    "- Input Layer: encoded characters\n",
    "- LSTM Layers: where the model learns patterns\n",
    "- Dense output layer: outputs a probability distribution over possible characters\n",
    "- Activation function: `softmax` convert logits to probabilities\n",
    "- `Adam` optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(seq_length, 1), return_sequences=True),\n",
    "    Dropout(0.2),  # Add dropout with a 20% drop rate\n",
    "    LSTM(256),\n",
    "    Dropout(0.2), \n",
    "    Dense(n_vocab),\n",
    "    Activation('softmax')\n",
    "])\n",
    "\n",
    "# Configure the model to use the Adam optimizer with a custom learning rate\n",
    "optimizer = Adam(learning_rate=0.001)  # Example learning rate, adjust based on performance\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training/Validating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "62/62 [==============================] - ETA: 0s - loss: 3.1010 - accuracy: 0.0869"
     ]
    }
   ],
   "source": [
    "# Model checkpoints and early stopping\n",
    "checkpoint = ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor=\"val_loss\")\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "# Train the model with the new setup\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val),\n",
    "                    callbacks=[checkpoint, early_stopping, reduce_lr])\n",
    "# Plotting the training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Generating Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(seed, length=20):\n",
    "    generated_name = seed\n",
    "    # Pad the seed to match the input length expected by the model\n",
    "    seed = seed.ljust(seq_length)\n",
    "\n",
    "    for _ in range(length):\n",
    "        # Convert the seed string to integer encoding and reshape for the model\n",
    "        x_pred = np.reshape([char_to_int[char] for char in seed], (1, seq_length, 1))\n",
    "        x_pred = x_pred / float(n_vocab)\n",
    "        \n",
    "        # Predict the next character\n",
    "        prediction = model.predict(x_pred, verbose=0)\n",
    "        index = np.argmax(prediction[-1])\n",
    "        result = int_to_char[index]\n",
    "        \n",
    "        # Append the result to the generated name and update the seed\n",
    "        generated_name += result\n",
    "        seed = seed[1:] + result\n",
    "\n",
    "    return generated_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "super  nnnnnnnnnnn       \n"
     ]
    }
   ],
   "source": [
    "# Example of generating a new name\n",
    "new_name = generate_name('super', 20)\n",
    "print(new_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
